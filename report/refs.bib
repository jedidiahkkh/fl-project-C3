@inproceedings{abadiDeepLearningDifferential2016,
  title = {Deep {{Learning}} with {{Differential Privacy}}},
  booktitle = {Proceedings of the 2016 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Abadi, Mart{\'i}n and Chu, Andy and Goodfellow, Ian and McMahan, H. Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  year = {2016},
  month = oct,
  eprint = {1607.00133},
  primaryclass = {cs, stat},
  pages = {308--318},
  doi = {10.1145/2976749.2978318},
  url = {http://arxiv.org/abs/1607.00133},
  urldate = {2024-02-20},
  abstract = {Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Jedidiah\\Zotero\\storage\\ELGRB25R\\Abadi et al. - 2016 - Deep Learning with Differential Privacy.pdf;C\:\\Users\\Jedidiah\\Zotero\\storage\\4I3M8J3R\\1607.html}
}

@misc{acarFederatedLearningBased2021,
  title = {Federated {{Learning Based}} on {{Dynamic Regularization}}},
  author = {Acar, Durmus Alp Emre and Zhao, Yue and Navarro, Ramon Matas and Mattina, Matthew and Whatmough, Paul N. and Saligrama, Venkatesh},
  year = {2021},
  month = nov,
  number = {arXiv:2111.04263},
  eprint = {2111.04263},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2111.04263},
  url = {http://arxiv.org/abs/2111.04263},
  urldate = {2024-03-28},
  abstract = {We propose a novel federated learning method for distributively training neural network models, where the server orchestrates cooperation between a subset of randomly chosen devices in each round. We view Federated Learning problem primarily from a communication perspective and allow more device level computations to save transmission costs. We point out a fundamental dilemma, in that the minima of the local-device level empirical loss are inconsistent with those of the global empirical loss. Different from recent prior works, that either attempt inexact minimization or utilize devices for parallelizing gradient computation, we propose a dynamic regularizer for each device at each round, so that in the limit the global and device solutions are aligned. We demonstrate both through empirical results on real and synthetic data as well as analytical results that our scheme leads to efficient training, in both convex and non-convex settings, while being fully agnostic to device heterogeneity and robust to large number of devices, partial participation and unbalanced data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning},
  file = {C\:\\Users\\Jedidiah\\Zotero\\storage\\GIC8QG9M\\Acar et al. - 2021 - Federated Learning Based on Dynamic Regularization.pdf;C\:\\Users\\Jedidiah\\Zotero\\storage\\5IPKDW4Y\\2111.html}
}

@book{akwaboahConvolutionalNeuralNetwork2019,
  title = {Convolutional {{Neural Network}} for {{CIFAR-10 Dataset Image Classification}}},
  author = {Akwaboah, Akwasi},
  year = {2019},
  month = nov,
  abstract = {Traditional neural networks though have achieved appreciable performance at image classification, they have been characterized by feature engineering, a tedious process that results in poor generalization to test data. In this report, I present a convolutional neural network (CNN) approach for classifying CIFAR-10 datasets. This approach has been shown in previous works to achieve improved performances without feature engineering. Learnable filters and pooling layers were used to extract underlying image features. Dropout, regularization along with variation in convolution strategies were applied to reduce overfitting while ensuring increased accuracies in validation and testing. Better test accuracy with reduced overfitting was achieved with a deeper network.}
}

@misc{andrewDifferentiallyPrivateLearning2022,
  title = {Differentially {{Private Learning}} with {{Adaptive Clipping}}},
  author = {Andrew, Galen and Thakkar, Om and McMahan, H. Brendan and Ramaswamy, Swaroop},
  year = {2022},
  month = may,
  number = {arXiv:1905.03871},
  eprint = {1905.03871},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/1905.03871},
  urldate = {2024-02-27},
  abstract = {Existing approaches for training neural networks with user-level differential privacy (e.g., DP Federated Averaging) in federated learning (FL) settings involve bounding the contribution of each user's model update by clipping it to some constant value. However there is no good a priori setting of the clipping norm across tasks and learning settings: the update norm distribution depends on the model architecture and loss, the amount of data on each device, the client learning rate, and possibly various other parameters. We propose a method wherein instead of a fixed clipping norm, one clips to a value at a specified quantile of the update norm distribution, where the value at the quantile is itself estimated online, with differential privacy. The method tracks the quantile closely, uses a negligible amount of privacy budget, is compatible with other federated learning technologies such as compression and secure aggregation, and has a straightforward joint DP analysis with DP-FedAvg. Experiments demonstrate that adaptive clipping to the median update norm works well across a range of realistic federated learning tasks, sometimes outperforming even the best fixed clip chosen in hindsight, and without the need to tune any clipping hyperparameter.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Jedidiah\\Zotero\\storage\\NL5BI6LM\\Andrew et al. - 2022 - Differentially Private Learning with Adaptive Clip.pdf;C\:\\Users\\Jedidiah\\Zotero\\storage\\Q74MNNU9\\1905.html}
}

@article{bleiLatentDirichletAllocation2003a,
  title = {Latent {{Dirichlet Allocation}}},
  author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
  year = {2003},
  journal = {Journal of Machine Learning Research},
  volume = {3},
  number = {Jan},
  pages = {993--1022},
  issn = {ISSN 1533-7928},
  url = {https://jmlr.csail.mit.edu/papers/v3/blei03a},
  urldate = {2024-03-30},
  abstract = {We describe latent Dirichlet allocation (LDA), a generative   probabilistic model for collections of discrete data such as text   corpora.  LDA is a three-level hierarchical Bayesian model, in which   each item of a collection is modeled as a finite mixture over an   underlying set of topics.  Each topic is, in turn, modeled as an   infinite mixture over an underlying set of topic probabilities.  In   the context of text modeling, the topic probabilities provide an   explicit representation of a document.  We present efficient   approximate inference techniques based on variational methods and an   EM algorithm for empirical Bayes parameter estimation.  We report   results in document modeling, text classification, and collaborative   filtering, comparing to a mixture of unigrams model and the   probabilistic LSI model.},
  file = {C:\Users\Jedidiah\Zotero\storage\HAA4XRX4\Blei et al. - 2003 - Latent Dirichlet Allocation.pdf}
}

@misc{bonawitzPracticalSecureAggregation2016,
  title = {Practical {{Secure Aggregation}} for {{Federated Learning}} on {{User-Held Data}}},
  author = {Bonawitz, Keith and Ivanov, Vladimir and Kreuter, Ben and Marcedone, Antonio and McMahan, H. Brendan and Patel, Sarvar and Ramage, Daniel and Segal, Aaron and Seth, Karn},
  year = {2016},
  month = nov,
  number = {arXiv:1611.04482},
  eprint = {1611.04482},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/1611.04482},
  urldate = {2024-02-27},
  abstract = {Secure Aggregation protocols allow a collection of mutually distrust parties, each holding a private value, to collaboratively compute the sum of those values without revealing the values themselves. We consider training a deep neural network in the Federated Learning model, using distributed stochastic gradient descent across user-held training data on mobile devices, wherein Secure Aggregation protects each user's model gradient. We design a novel, communication-efficient Secure Aggregation protocol for high-dimensional data that tolerates up to 1/3 users failing to complete the protocol. For 16-bit input values, our protocol offers 1.73x communication expansion for \$2\^{}\{10\}\$ users and \$2\^{}\{20\}\$-dimensional vectors, and 1.98x expansion for \$2\^{}\{14\}\$ users and \$2\^{}\{24\}\$ dimensional vectors.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Cryptography and Security,Statistics - Machine Learning},
  file = {C\:\\Users\\Jedidiah\\Zotero\\storage\\5SYLWYPL\\Bonawitz et al. - 2016 - Practical Secure Aggregation for Federated Learnin.pdf;C\:\\Users\\Jedidiah\\Zotero\\storage\\Z7TPK8NH\\1611.html}
}

@inproceedings{chattopadhyayROFLRObustPrivacy2022,
  title = {{{ROFL}}: {{RObust}} Privacy Preserving {{Federated Learning}}},
  shorttitle = {{{ROFL}}},
  booktitle = {2022 {{IEEE}} 42nd {{International Conference}} on {{Distributed Computing Systems Workshops}} ({{ICDCSW}})},
  author = {Chattopadhyay, Nandish and Singh, Arpit and Chattopadhyay, Anupam},
  year = {2022},
  month = jul,
  pages = {125--132},
  issn = {2332-5666},
  doi = {10.1109/ICDCSW56584.2022.00033},
  url = {https://ieeexplore.ieee.org/abstract/document/9951361},
  urldate = {2024-02-20},
  abstract = {In the modern world of connectivity, most data is generated in a de centralised way, across a multitude of platforms like mobile devices and other loT applications. This crowd sourced data, if well analyzed, can prove to be rich in insights, for different tasks. However, the issue in utilizing it lies with the consolidation of the data, which is unacceptable to most involved parties. While every participant stands to benefit from the collective use of the massive data repositories, the lack of trust between them prevents that endeavour. In this paper, we propose ROFL, which is an end-to-end robust mechanism of learning, that has been developed keeping all the trust issues in mind and addressing the necessity of privacy. We make note of the threat models that might make the participants apprehensive and design a bi-directional two-dimensional privacy preserving framework, that builds upon the state-of-the-art in differentially private federated learning. Specifically, we propose a weighted federated averaging technique for aggregation of the differentially private models generated by the participants. We are able to provide privacy guarantees without compromising on the accuracy of the machine learning task. ROFL has been tested for multiple neural architectures (VGG-16 [1] and ResNet [2]) on multiple datasets (MNIST [3], CIFAR-I0 and CIFAR-I00 [4]). On the machine learning tasks, it is able to achieve accuracies within the range of 1 \% -2 \% of what a model trained on the collected data would have generated, in the average case scenario. We have verified the robustness of ROFL against attacks involving sabotaging or malicious client providing erroneous models. The study on model convergence reveals how to improve the efficiency of ROFL. We also provide evidence on how ROFL is easily scalable in nature.},
  keywords = {Computational modeling,Data models,Deep learning,Differential Privacy,Federated learning,Federated Learning,Internet of Things,Mobile handsets,Privacy,Robustness,Threat modeling},
  file = {C\:\\Users\\Jedidiah\\Zotero\\storage\\UM4MEX8U\\Chattopadhyay et al. - 2022 - ROFL RObust privacy preserving Federated Learning.pdf;C\:\\Users\\Jedidiah\\Zotero\\storage\\G39EZ32P\\9951361.html}
}

@misc{ConvolutionalNeuralNetwork2024,
  title = {Convolutional {{Neural Network}} ({{CNN}}) {\textbar} {{TensorFlow Core}}},
  year = {2024},
  url = {https://www.tensorflow.org/tutorials/images/cnn},
  urldate = {2024-03-28},
  abstract = {Complete, end-to-end examples to learn how to use TensorFlow for ML beginners and experts. Try tutorials in Google Colab - no setup required.},
  langid = {english},
  file = {C:\Users\Jedidiah\Zotero\storage\57CHNJQV\cnn.html}
}

@misc{dinhSharpMinimaCan2017,
  title = {Sharp {{Minima Can Generalize For Deep Nets}}},
  author = {Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
  year = {2017},
  month = may,
  number = {arXiv:1703.04933},
  eprint = {1703.04933},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/1703.04933},
  urldate = {2024-03-14},
  abstract = {Despite their overwhelming capacity to overfit, deep learning architectures tend to generalize relatively well to unseen data, allowing them to be deployed in practice. However, explaining why this is the case is still an open area of research. One standing hypothesis that is gaining popularity, e.g. Hochreiter \& Schmidhuber (1997); Keskar et al. (2017), is that the flatness of minima of the loss function found by stochastic gradient based methods results in good generalization. This paper argues that most notions of flatness are problematic for deep models and can not be directly applied to explain generalization. Specifically, when focusing on deep networks with rectifier units, we can exploit the particular geometry of parameter space induced by the inherent symmetries that these architectures exhibit to build equivalent models corresponding to arbitrarily sharper minima. Furthermore, if we allow to reparametrize a function, the geometry of its parameters can change drastically without affecting its generalization properties.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\Jedidiah\\Zotero\\storage\\I2ZJZFGH\\Dinh et al. - 2017 - Sharp Minima Can Generalize For Deep Nets.pdf;C\:\\Users\\Jedidiah\\Zotero\\storage\\698RKGMY\\1703.html}
}

@article{duchiAdaptiveSubgradientMethods2011,
  title = {Adaptive {{Subgradient Methods}} for {{Online Learning}} and {{Stochastic Optimization}}},
  author = {Duchi, John and Hazan, Elad and Singer, Yoram},
  year = {2011},
  journal = {Journal of Machine Learning Research},
  volume = {12},
  number = {61},
  pages = {2121--2159},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v12/duchi11a.html},
  urldate = {2024-03-31},
  abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
  file = {C:\Users\Jedidiah\Zotero\storage\HTKHYM52\Duchi et al. - 2011 - Adaptive Subgradient Methods for Online Learning a.pdf}
}

@misc{fallahPersonalizedFederatedLearning2020,
  title = {Personalized {{Federated Learning}}: {{A Meta-Learning Approach}}},
  shorttitle = {Personalized {{Federated Learning}}},
  author = {Fallah, Alireza and Mokhtari, Aryan and Ozdaglar, Asuman},
  year = {2020},
  month = oct,
  number = {arXiv:2002.07948},
  eprint = {2002.07948},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2002.07948},
  url = {http://arxiv.org/abs/2002.07948},
  urldate = {2024-03-31},
  abstract = {In Federated Learning, we aim to train models across multiple computing units (users), while users can only communicate with a common central server, without exchanging their data samples. This mechanism exploits the computational power of all users and allows users to obtain a richer model as their models are trained over a larger set of data points. However, this scheme only develops a common output for all the users, and, therefore, it does not adapt the model to each user. This is an important missing feature, especially given the heterogeneity of the underlying data distribution for various users. In this paper, we study a personalized variant of the federated learning in which our goal is to find an initial shared model that current or new users can easily adapt to their local dataset by performing one or a few steps of gradient descent with respect to their own data. This approach keeps all the benefits of the federated learning architecture, and, by structure, leads to a more personalized model for each user. We show this problem can be studied within the Model-Agnostic Meta-Learning (MAML) framework. Inspired by this connection, we study a personalized variant of the well-known Federated Averaging algorithm and evaluate its performance in terms of gradient norm for non-convex loss functions. Further, we characterize how this performance is affected by the closeness of underlying distributions of user data, measured in terms of distribution distances such as Total Variation and 1-Wasserstein metric.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,NLP example,Statistics - Machine Learning},
  file = {C\:\\Users\\Jedidiah\\Zotero\\storage\\Y796RIEW\\Fallah et al. - 2020 - Personalized Federated Learning A Meta-Learning A.pdf;C\:\\Users\\Jedidiah\\Zotero\\storage\\G7Y9NLI2\\2002.html}
}

@inproceedings{frigyikIntroductionDirichletDistribution2010,
  title = {Introduction to the {{Dirichlet Distribution}} and {{Related Processes}}},
  author = {Frigyik, Andrew B{\'e}la and Kapila, A. and Gupta, M.},
  year = {2010},
  url = {https://www.semanticscholar.org/paper/Introduction-to-the-Dirichlet-Distribution-and-Frigyik-Kapila/775e5727f5df0cb9bf834af2ea2548a696c27a38},
  urldate = {2024-03-31},
  abstract = {This tutorial covers the Dirichlet distribution, Dirichlet process, P{\'o}lya urn (and the associated Chinese restaurant process), hierarchical Dirichlet Process, and the Indian buffet process. Apart from basic properties, we describe and contrast three methods of generating samples: stick-breaking, the P{\'o}lya urn, and drawing gamma random variables. For the Dirichlet process we first present an informal introduction, and then a rigorous description for those more comfortable with probability theory.},
  file = {C:\Users\Jedidiah\Zotero\storage\WBREU3UT\Frigyik et al. - 2010 - Introduction to the Dirichlet Distribution and Rel.pdf}
}

@misc{karimireddySCAFFOLDStochasticControlled2021,
  title = {{{SCAFFOLD}}: {{Stochastic Controlled Averaging}} for {{Federated Learning}}},
  shorttitle = {{{SCAFFOLD}}},
  author = {Karimireddy, Sai Praneeth and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank J. and Stich, Sebastian U. and Suresh, Ananda Theertha},
  year = {2021},
  month = apr,
  number = {arXiv:1910.06378},
  eprint = {1910.06378},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1910.06378},
  url = {http://arxiv.org/abs/1910.06378},
  urldate = {2024-03-30},
  abstract = {Federated Averaging (FedAvg) has emerged as the algorithm of choice for federated learning due to its simplicity and low communication cost. However, in spite of recent research efforts, its performance is not fully understood. We obtain tight convergence rates for FedAvg and prove that it suffers from `client-drift' when the data is heterogeneous (non-iid), resulting in unstable and slow convergence. As a solution, we propose a new algorithm (SCAFFOLD) which uses control variates (variance reduction) to correct for the `client-drift' in its local updates. We prove that SCAFFOLD requires significantly fewer communication rounds and is not affected by data heterogeneity or client sampling. Further, we show that (for quadratics) SCAFFOLD can take advantage of similarity in the client's data yielding even faster convergence. The latter is the first result to quantify the usefulness of local-steps in distributed optimization.},
  archiveprefix = {arxiv},
  keywords = {68W40 68W15 90C25 90C06,Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning,E.4,F.2.1,G.1.6,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {C\:\\Users\\Jedidiah\\Zotero\\storage\\LB78ETQX\\Karimireddy et al. - 2021 - SCAFFOLD Stochastic Controlled Averaging for Fede.pdf;C\:\\Users\\Jedidiah\\Zotero\\storage\\NPVIZCZS\\1910.html}
}

@misc{keskarLargeBatchTrainingDeep2017,
  title = {On {{Large-Batch Training}} for {{Deep Learning}}: {{Generalization Gap}} and {{Sharp Minima}}},
  shorttitle = {On {{Large-Batch Training}} for {{Deep Learning}}},
  author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  year = {2017},
  month = feb,
  number = {arXiv:1609.04836},
  eprint = {1609.04836},
  primaryclass = {cs, math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1609.04836},
  url = {http://arxiv.org/abs/1609.04836},
  urldate = {2024-03-14},
  abstract = {The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say \$32\$-\$512\$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions - and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control},
  file = {C\:\\Users\\Jedidiah\\Zotero\\storage\\NV3ICP73\\Keskar et al. - 2017 - On Large-Batch Training for Deep Learning General.pdf;C\:\\Users\\Jedidiah\\Zotero\\storage\\75IGXVXV\\1609.html}
}

@misc{kingmaAdamMethodStochastic2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2017},
  month = jan,
  number = {arXiv:1412.6980},
  eprint = {1412.6980},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1412.6980},
  url = {http://arxiv.org/abs/1412.6980},
  urldate = {2024-03-31},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\Jedidiah\\Zotero\\storage\\QXU8DQWR\\Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf;C\:\\Users\\Jedidiah\\Zotero\\storage\\EJSAQ5W4\\1412.html}
}

@misc{konecnyFederatedOptimizationDistributed2016,
  title = {Federated {{Optimization}}: {{Distributed Machine Learning}} for {{On-Device Intelligence}}},
  shorttitle = {Federated {{Optimization}}},
  author = {Kone{\v c}n{\'y}, Jakub and McMahan, H. Brendan and Ramage, Daniel and Richt{\'a}rik, Peter},
  year = {2016},
  month = oct,
  number = {arXiv:1610.02527},
  eprint = {1610.02527},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1610.02527},
  url = {http://arxiv.org/abs/1610.02527},
  urldate = {2024-03-30},
  abstract = {We introduce a new and increasingly relevant setting for distributed optimization in machine learning, where the data defining the optimization are unevenly distributed over an extremely large number of nodes. The goal is to train a high-quality centralized model. We refer to this setting as Federated Optimization. In this setting, communication efficiency is of the utmost importance and minimizing the number of rounds of communication is the principal goal. A motivating example arises when we keep the training data locally on users' mobile devices instead of logging it to a data center for training. In federated optimziation, the devices are used as compute nodes performing computation on their local data in order to update a global model. We suppose that we have extremely large number of devices in the network --- as many as the number of users of a given service, each of which has only a tiny fraction of the total data available. In particular, we expect the number of data points available locally to be much smaller than the number of devices. Additionally, since different users generate data with different patterns, it is reasonable to assume that no device has a representative sample of the overall distribution. We show that existing algorithms are not suitable for this setting, and propose a new algorithm which shows encouraging experimental results for sparse convex problems. This work also sets a path for future research needed in the context of {\textbackslash}federated optimization.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\Jedidiah\\Zotero\\storage\\55RBA5LB\\Konečný et al. - 2016 - Federated Optimization Distributed Machine Learni.pdf;C\:\\Users\\Jedidiah\\Zotero\\storage\\K9PXB7MH\\1610.html}
}

@article{krizhevskyLearningMultipleLayers2009,
  title = {Learning {{Multiple Layers}} of {{Features}} from {{Tiny Images}}},
  author = {Krizhevsky, Alex and Hinton, Geoffrey},
  year = {2009},
  publisher = {Toronto, ON, Canada},
  langid = {english},
  file = {C:\Users\Jedidiah\Zotero\storage\WZHWA44W\Krizhevsky - Learning Multiple Layers of Features from Tiny Ima.pdf}
}

@misc{liFederatedOptimizationHeterogeneous2020,
  title = {Federated {{Optimization}} in {{Heterogeneous Networks}}},
  author = {Li, Tian and Sahu, Anit Kumar and Zaheer, Manzil and Sanjabi, Maziar and Talwalkar, Ameet and Smith, Virginia},
  year = {2020},
  month = apr,
  number = {arXiv:1812.06127},
  eprint = {1812.06127},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1812.06127},
  url = {http://arxiv.org/abs/1812.06127},
  urldate = {2024-03-30},
  abstract = {Federated Learning is a distributed learning paradigm with two key challenges that differentiate it from traditional distributed optimization: (1) significant variability in terms of the systems characteristics on each device in the network (systems heterogeneity), and (2) non-identically distributed data across the network (statistical heterogeneity). In this work, we introduce a framework, FedProx, to tackle heterogeneity in federated networks. FedProx can be viewed as a generalization and re-parametrization of FedAvg, the current state-of-the-art method for federated learning. While this re-parameterization makes only minor modifications to the method itself, these modifications have important ramifications both in theory and in practice. Theoretically, we provide convergence guarantees for our framework when learning over data from non-identical distributions (statistical heterogeneity), and while adhering to device-level systems constraints by allowing each participating device to perform a variable amount of work (systems heterogeneity). Practically, we demonstrate that FedProx allows for more robust convergence than FedAvg across a suite of realistic federated datasets. In particular, in highly heterogeneous settings, FedProx demonstrates significantly more stable and accurate convergence behavior relative to FedAvg---improving absolute test accuracy by 22\% on average.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Jedidiah\\Zotero\\storage\\T79ZHQDI\\Li et al. - 2020 - Federated Optimization in Heterogeneous Networks.pdf;C\:\\Users\\Jedidiah\\Zotero\\storage\\FAIV7KAS\\1812.html}
}

@inproceedings{lycklamaRoFLRobustnessSecure2023,
  title = {{{RoFL}}: {{Robustness}} of {{Secure Federated Learning}}},
  shorttitle = {{{RoFL}}},
  booktitle = {2023 {{IEEE Symposium}} on {{Security}} and {{Privacy}} ({{SP}})},
  author = {Lycklama, Hidde and Burkhalter, Lukas and Viand, Alexander and K{\"u}chler, Nicolas and Hithnawi, Anwar},
  year = {2023},
  month = may,
  pages = {453--476},
  issn = {2375-1207},
  doi = {10.1109/SP46215.2023.10179400},
  url = {https://ieeexplore.ieee.org/abstract/document/10179400},
  urldate = {2024-02-20},
  abstract = {Even though recent years have seen many attacks exposing severe vulnerabilities in Federated Learning (FL), a holistic understanding of what enables these attacks and how they can be mitigated effectively is still lacking. In this work, we demystify the inner workings of existing (targeted) attacks. We provide new insights into why these attacks are possible and why a definitive solution to FL robustness is challenging. We show that the need for ML algorithms to memorize tail data has significant implications for FL integrity. This phenomenon has largely been studied in the context of privacy; our analysis sheds light on its implications for ML integrity. We show that certain classes of severe attacks can be mitigated effectively by enforcing constraints such as norm bounds on clients' updates. We investigate how to efficiently incorporate these constraints into secure FL protocols in the single-server setting. Based on this, we propose RoFL, a new secure FL system that extends secure aggregation with privacy-preserving input validation. Specifically, RoFL can enforce constraints such as L2 and L{$\infty$} bounds on high-dimensional encrypted model updates.},
  keywords = {Aggregates,Bandwidth,Federated learning,federated-learning,Privacy,privacy-preserving-machine-learning,Protocols,Scalability,secure-aggregation,Tail},
  file = {C:\Users\Jedidiah\Zotero\storage\UHSXEMZY\Lycklama et al. - 2023 - RoFL Robustness of Secure Federated Learning.pdf}
}

@inproceedings{mcmahanCommunicationEfficientLearningDeep2017,
  title = {Communication-{{Efficient Learning}} of {{Deep Networks}} from {{Decentralized Data}}},
  booktitle = {Proceedings of the 20th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Ag{\"u}era},
  year = {2017},
  month = apr,
  pages = {1273--1282},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v54/mcmahan17a.html},
  urldate = {2024-03-30},
  abstract = {Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches.  We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning.  We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.},
  langid = {english},
  file = {C:\Users\Jedidiah\Zotero\storage\MGJ64S8U\McMahan et al. - 2017 - Communication-Efficient Learning of Deep Networks .pdf}
}

@misc{mcmahanCommunicationEfficientLearningDeep2023,
  title = {Communication-{{Efficient Learning}} of {{Deep Networks}} from {{Decentralized Data}}},
  author = {McMahan, H. Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Ag{\"u}era},
  year = {2023},
  month = jan,
  number = {arXiv:1602.05629},
  eprint = {1602.05629},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/1602.05629},
  urldate = {2024-03-28},
  abstract = {Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\Jedidiah\\Zotero\\storage\\PL6WV8CP\\McMahan et al. - 2023 - Communication-Efficient Learning of Deep Networks .pdf;C\:\\Users\\Jedidiah\\Zotero\\storage\\J48EQTC5\\1602.html}
}

@misc{mcmahanLearningDifferentiallyPrivate2018,
  title = {Learning {{Differentially Private Recurrent Language Models}}},
  author = {McMahan, H. Brendan and Ramage, Daniel and Talwar, Kunal and Zhang, Li},
  year = {2018},
  month = feb,
  number = {arXiv:1710.06963},
  eprint = {1710.06963},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/1710.06963},
  urldate = {2024-02-27},
  abstract = {We demonstrate that it is possible to train large recurrent language models with user-level differential privacy guarantees with only a negligible cost in predictive accuracy. Our work builds on recent advances in the training of deep networks on user-partitioned data and privacy accounting for stochastic gradient descent. In particular, we add user-level privacy protection to the federated averaging algorithm, which makes "large step" updates from user-level data. Our work demonstrates that given a dataset with a sufficiently large number of users (a requirement easily met by even small internet-scale datasets), achieving differential privacy comes at the cost of increased computation, rather than in decreased utility as in most prior work. We find that our private LSTM language models are quantitatively and qualitatively similar to un-noised models when trained on a large dataset.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\Jedidiah\\Zotero\\storage\\BY6JVL45\\McMahan et al. - 2018 - Learning Differentially Private Recurrent Language.pdf;C\:\\Users\\Jedidiah\\Zotero\\storage\\SH6WHZZA\\1710.html}
}

@misc{moCentaurFederatedLearning2022,
  title = {Centaur: {{Federated Learning}} for {{Constrained Edge Devices}}},
  shorttitle = {Centaur},
  author = {Mo, Fan and Malekzadeh, Mohammad and Chatterjee, Soumyajit and Kawsar, Fahim and Mathur, Akhil},
  year = {2022},
  month = nov,
  number = {arXiv:2211.04175},
  eprint = {2211.04175},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.04175},
  url = {http://arxiv.org/abs/2211.04175},
  urldate = {2024-02-22},
  abstract = {Federated learning (FL) on deep neural networks facilitates new applications at the edge, especially for wearable and Internet-of-Thing devices. Such devices capture a large and diverse amount of data, but they have memory, compute, power, and connectivity constraints which hinder their participation in FL. We propose Centaur, a multitier FL framework, enabling ultra-constrained devices to efficiently participate in FL on large neural nets. Centaur combines two major ideas: (i) a data selection scheme to choose a portion of samples that accelerates the learning, and (ii) a partition-based training algorithm that integrates both constrained and powerful devices owned by the same user. Evaluations, on four benchmark neural nets and three datasets, show that Centaur gains {\textasciitilde}10\% higher accuracy than local training on constrained devices with {\textasciitilde}58\% energy saving on average. Our experimental results also demonstrate the superior efficiency of Centaur when dealing with imbalanced data, client participation heterogeneity, and various network connection probabilities.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\Jedidiah\\Zotero\\storage\\GSVRAVIR\\Mo et al. - 2022 - Centaur Federated Learning for Constrained Edge D.pdf;C\:\\Users\\Jedidiah\\Zotero\\storage\\UXNZVNZQ\\2211.html}
}

@misc{reddiAdaptiveFederatedOptimization2021,
  title = {Adaptive {{Federated Optimization}}},
  author = {Reddi, Sashank and Charles, Zachary and Zaheer, Manzil and Garrett, Zachary and Rush, Keith and Kone{\v c}n{\'y}, Jakub and Kumar, Sanjiv and McMahan, H. Brendan},
  year = {2021},
  month = sep,
  number = {arXiv:2003.00295},
  eprint = {2003.00295},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2003.00295},
  urldate = {2024-03-06},
  abstract = {Federated learning is a distributed machine learning paradigm in which a large number of clients coordinate with a central server to learn a model without sharing their own training data. Standard federated optimization methods such as Federated Averaging (FedAvg) are often difficult to tune and exhibit unfavorable convergence behavior. In non-federated settings, adaptive optimization methods have had notable success in combating such issues. In this work, we propose federated versions of adaptive optimizers, including Adagrad, Adam, and Yogi, and analyze their convergence in the presence of heterogeneous data for general non-convex settings. Our results highlight the interplay between client heterogeneity and communication efficiency. We also perform extensive experiments on these methods and show that the use of adaptive optimizers can significantly improve the performance of federated learning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {C\:\\Users\\Jedidiah\\Zotero\\storage\\URE27LSF\\Reddi et al. - 2021 - Adaptive Federated Optimization.pdf;C\:\\Users\\Jedidiah\\Zotero\\storage\\868ESZXP\\2003.html}
}

@misc{ValueMeanStd2017,
  title = {The Value of Mean, Std in Cifar-10 {$\cdot$} {{Issue}} \#180 {$\cdot$} Facebookarchive/Fb.Resnet.Torch},
  year = {2017},
  journal = {GitHub},
  url = {https://github.com/facebookarchive/fb.resnet.torch/issues/180},
  urldate = {2024-03-28},
  abstract = {meanstd = \{ mean = \{125.3, 123.0, 113.9\}, std = \{63.0, 62.1, 66.7\}, \} is these values are in rgb or bgr order??},
  langid = {english},
  file = {C:\Users\Jedidiah\Zotero\storage\JEIDIGZ6\180.html}
}

@misc{viandVerifiableFullyHomomorphic2023,
  title = {Verifiable {{Fully Homomorphic Encryption}}},
  author = {Viand, Alexander and Knabenhans, Christian and Hithnawi, Anwar},
  year = {2023},
  month = feb,
  number = {arXiv:2301.07041},
  eprint = {2301.07041},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2301.07041},
  url = {http://arxiv.org/abs/2301.07041},
  urldate = {2024-02-20},
  abstract = {Fully Homomorphic Encryption (FHE) is seeing increasing real-world deployment to protect data in use by allowing computation over encrypted data. However, the same malleability that enables homomorphic computations also raises integrity issues, which have so far been mostly overlooked. While FHEs lack of integrity has obvious implications for correctness, it also has severe implications for confidentiality: a malicious server can leverage the lack of integrity to carry out interactive key-recovery attacks. As a result, virtually all FHE schemes and applications assume an honest-but-curious server who does not deviate from the protocol. In practice, however, this assumption is insufficient for a wide range of deployment scenarios. While there has been work that aims to address this gap, these have remained isolated efforts considering only aspects of the overall problem and fail to fully address the needs and characteristics of modern FHE schemes and applications. In this paper, we analyze existing FHE integrity approaches, present attacks that exploit gaps in prior work, and propose a new notion for maliciously-secure verifiable FHE. We then instantiate this new notion with a range of techniques, analyzing them and evaluating their performance in a range of different settings. We highlight their potential but also show where future work on tailored integrity solutions for FHE is still required.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Cryptography and Security},
  file = {C\:\\Users\\Jedidiah\\Zotero\\storage\\CDZ8P8RE\\Viand et al. - 2023 - Verifiable Fully Homomorphic Encryption.pdf;C\:\\Users\\Jedidiah\\Zotero\\storage\\993Z4D6B\\2301.html}
}

@inproceedings{zaheerAdaptiveMethodsNonconvex2018,
  title = {Adaptive {{Methods}} for {{Nonconvex Optimization}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zaheer, Manzil and Reddi, Sashank and Sachan, Devendra and Kale, Satyen and Kumar, Sanjiv},
  year = {2018},
  volume = {31},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2018/hash/90365351ccc7437a1309dc64e4db32a3-Abstract.html},
  urldate = {2024-03-31},
  abstract = {Adaptive gradient methods that rely on scaling gradients down by the square root of exponential moving averages of past squared gradients, such RMSProp, Adam, Adadelta have found wide application in optimizing the nonconvex problems that arise in deep learning. However, it has been recently demonstrated that such methods can fail to converge even in simple convex optimization settings. In this work, we provide a new analysis of such methods applied to nonconvex stochastic optimization problems, characterizing the effect of increasing minibatch size. Our analysis shows that under this scenario such methods do converge to stationarity up to the statistical limit of variance in the stochastic gradients (scaled by a constant factor). In particular, our result implies that increasing minibatch sizes enables convergence,  thus providing a way to circumvent the non-convergence issues. Furthermore, we provide a new adaptive optimization algorithm, Yogi, which controls the increase in effective learning rate,  leading to even better performance with similar theoretical guarantees on convergence. Extensive experiments show that Yogi with very little hyperparameter tuning outperforms methods such as Adam in several challenging machine learning tasks.},
  file = {C:\Users\Jedidiah\Zotero\storage\Z8C8C5FL\Zaheer et al. - 2018 - Adaptive Methods for Nonconvex Optimization.pdf}
}

@article{zhaoFederatedLearningNonIID2018,
  title = {Federated {{Learning}} with {{Non-IID Data}}},
  author = {Zhao, Yue and Li, Meng and Lai, Liangzhen and Suda, Naveen and Civin, Damon and Chandra, Vikas},
  year = {2018},
  eprint = {1806.00582},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1806.00582},
  url = {http://arxiv.org/abs/1806.00582},
  urldate = {2024-03-31},
  abstract = {Federated learning enables resource-constrained edge compute devices, such as mobile phones and IoT devices, to learn a shared model for prediction, while keeping the training data local. This decentralized approach to train models provides privacy, security, regulatory and economic benefits. In this work, we focus on the statistical challenge of federated learning when local data is non-IID. We first show that the accuracy of federated learning reduces significantly, by up to 55\% for neural networks trained for highly skewed non-IID data, where each client device trains only on a single class of data. We further show that this accuracy reduction can be explained by the weight divergence, which can be quantified by the earth mover's distance (EMD) between the distribution over classes on each device and the population distribution. As a solution, we propose a strategy to improve training on non-IID data by creating a small subset of data which is globally shared between all the edge devices. Experiments show that accuracy can be increased by 30\% for the CIFAR-10 dataset with only 5\% globally shared data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Jedidiah\\Zotero\\storage\\AZ7RS6TQ\\Zhao et al. - 2018 - Federated Learning with Non-IID Data.pdf;C\:\\Users\\Jedidiah\\Zotero\\storage\\6USTSBRC\\1806.html}
}

@article{zhouFedFAFederatedLearning2024,
  title = {{{FedFA}}: {{Federated Learning}} with {{Feature Anchors}} to {{Align Features}} and {{Classifiers}} for {{Heterogeneous Data}}},
  shorttitle = {{{FedFA}}},
  author = {Zhou, Tailin and Zhang, Jun and Tsang, Danny H. K.},
  year = {2024},
  journal = {IEEE Transactions on Mobile Computing},
  eprint = {2211.09299},
  primaryclass = {cs},
  pages = {1--12},
  issn = {1536-1233, 1558-0660, 2161-9875},
  doi = {10.1109/TMC.2023.3325366},
  url = {http://arxiv.org/abs/2211.09299},
  urldate = {2024-03-28},
  abstract = {Federated learning allows multiple clients to collaboratively train a model without exchanging their data, thus preserving data privacy. Unfortunately, it suffers significant performance degradation due to heterogeneous data at clients. Common solutions involve designing an auxiliary loss to regularize weight divergence or feature inconsistency during local training. However, we discover that these approaches fall short of the expected performance because they ignore the existence of a vicious cycle between feature inconsistency and classifier divergence across clients. This vicious cycle causes client models to be updated in inconsistent feature spaces with more diverged classifiers. To break the vicious cycle, we propose a novel framework named Federated learning with Feature Anchors (FedFA). FedFA utilizes feature anchors to align features and calibrate classifiers across clients simultaneously. This enables client models to be updated in a shared feature space with consistent classifiers during local training. Theoretically, we analyze the non-convex convergence rate of FedFA. We also demonstrate that the integration of feature alignment and classifier calibration in FedFA brings a virtuous cycle between feature and classifier updates, which breaks the vicious cycle existing in current approaches. Extensive experiments show that FedFA significantly outperforms existing approaches on various classification datasets under label distribution skew and feature distribution skew.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\Jedidiah\\Zotero\\storage\\UK8NHGN9\\Zhou et al. - 2024 - FedFA Federated Learning with Feature Anchors to .pdf;C\:\\Users\\Jedidiah\\Zotero\\storage\\R9R8JLCH\\2211.html}
}

@misc{zhouUnderstandingImprovingModel2023,
  title = {Understanding and {{Improving Model Averaging}} in {{Federated Learning}} on {{Heterogeneous Data}}},
  author = {Zhou, Tailin and Lin, Zehong and Zhang, Jun and Tsang, Danny H. K.},
  year = {2023},
  month = oct,
  number = {arXiv:2305.07845},
  eprint = {2305.07845},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2305.07845},
  urldate = {2024-03-06},
  abstract = {Model averaging is a widely adopted technique in federated learning (FL) that aggregates multiple client models to obtain a global model. Remarkably, model averaging in FL can yield a superior global model, even when client models are trained with non-convex objective functions and on heterogeneous local datasets. However, the rationale behind its success remains poorly understood. To shed light on this issue, we first visualize the loss landscape of FL over client and global models to illustrate their geometric properties. The visualization shows that the client models encompass the global model within a common basin, and interestingly, the global model may deviate from the bottom of the basin while still outperforming the client models. To gain further insights into model averaging in FL, we decompose the expected loss of the global model into five factors related to the client models. Specifically, our analysis reveals that the loss of the global model after early training mainly arises from {\textbackslash}textit\{i)\} the client model's loss on non-overlapping data between client datasets and the global dataset and {\textbackslash}textit\{ii)\} the maximum distance between the global and client models. Based on these findings from our loss landscape visualization and loss decomposition, we propose utilizing iterative moving averaging (IMA) on the global model at the late training phase to reduce its deviation from the expected minimum, while constraining client exploration to limit the maximum distance between the global and client models. Our experiments demonstrate that incorporating IMA into existing FL methods significantly improves their accuracy and training speed on various heterogeneous data setups of benchmark datasets.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\Jedidiah\\Zotero\\storage\\VAFFLTJI\\Zhou et al. - 2023 - Understanding and Improving Model Averaging in Fed.pdf;C\:\\Users\\Jedidiah\\Zotero\\storage\\U84CVK9V\\2305.html}
}
